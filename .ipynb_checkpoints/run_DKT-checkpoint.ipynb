{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用生成的模拟数据进行实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from data import load_data\n",
    "# from deep_knowledge_tracing_model import DeepKnowledgeTracing\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from DKT_model import RNN\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "parser = argparse.ArgumentParser(description='DKT_LSTM model')\n",
    "parser.add_argument('-epsilon', type=float, default=0.1, help='Epsilon value for Adam Optimizer')\n",
    "parser.add_argument('-l2_lambda', type=float, default=0.3, help='Lambda for l2 loss')\n",
    "parser.add_argument('-learning_rate', type=float, default=0.002, help='Learning rate')\n",
    "parser.add_argument('-max_grad_norm', type=float, default=20, help='Clip gradients to this norm')\n",
    "parser.add_argument('-keep_prob', type=float, default=0.6, help='Keep probability for dropout')\n",
    "parser.add_argument('-hidden_layer_num', type=int, default=1, help='The number of hidden layers')\n",
    "parser.add_argument('-concept_num', type=int, default=5, help='The number of hidden nodes')\n",
    "parser.add_argument('-evaluation_interval', type=int, default=5, help='Evalutaion and print result every x epochs')\n",
    "parser.add_argument('-batch_size', type=int, default=32, help='Batch size for training')\n",
    "parser.add_argument('-epochs', type=int, default=100, help='Number of epochs to train')\n",
    "parser.add_argument('-allow_soft_placement', type=bool, default=True, help='Allow device soft device placement')\n",
    "parser.add_argument('-log_device_placement', type=bool, default=False, help='Log placement ofops on devices')\n",
    "# parser.add_argument('-train_data_path', type=str, default='./遗忘_学生500题目60概念数5/答题序列数据_train'+str(index)+'.csv', help='Path to the training dataset')\n",
    "# parser.add_argument('-test_data_path', type=str, default='./遗忘_学生500题目60概念数5/答题序列数据_test'+str(index)+'.csv',help='Path to the testing dataset')\n",
    "# parser.add_argument('-qm_data_path', type=str, default='./遗忘_学生500题目60概念数5/题目知识点数据'+str(index)+'.csv',help='path to read q_matrix')\n",
    "parser.add_argument('-train_data_path', type=str, default='./datasets/atc_datasets/s500_q10_kc5/s500_q10_kc5_train0.csv', help='Path to the training dataset')\n",
    "parser.add_argument('-test_data_path', type=str, default='./datasets/atc_datasets/s500_q10_kc5/s500_q10_kc5_test0.csv',help='Path to the testing dataset')\n",
    "parser.add_argument('-qm_data_path', type=str, default='./datasets/atc_datasets/s500_q10_kc5/qm_atc2.npy',help='path to read q_matrix')\n",
    "parser.add_argument('-origin_DKT', type=str, default=False,help='Path to the testing dataset')\n",
    "parser.add_argument('-hidden_size', type=int, default=10, help='The number of hidden nodes')\n",
    "\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(m, optimizer, students, batch_size, num_steps, num_skills, q_matrix, state_writer, training=True, epoch=1):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "\n",
    "    lr = args.learning_rate # learning rate\n",
    "    train_path = args.train_data_path\n",
    "    concept_num = args.concept_num\n",
    "    total_loss = 0\n",
    "    input_size = num_skills * 2\n",
    "    start_time = time.time()\n",
    "    index = 0\n",
    "    actual_labels = []\n",
    "    div_labels=[]\n",
    "    pred_labels = []\n",
    "    # hidden = m.init_hidden(batch_size)\n",
    "    count = 0\n",
    "#     batch_num = len(students) // batch_size\n",
    "    batch_num=0\n",
    "    total_auc = 0\n",
    "    time_list=[]\n",
    "    for i in range(num_steps):\n",
    "        time_list.append(i)\n",
    "#     print(\"数据分批次\")\n",
    "    while(index+batch_size < len(students)):\n",
    "        batch_num+=1\n",
    "        x = np.zeros((batch_size, num_steps))\n",
    "        target_id: List[int] = []\n",
    "        target_correctness = []\n",
    "        answer_print=[]\n",
    "        for i in range(batch_size):\n",
    "            #student的格式：(['4'], ['77', '77', '32', '32'], ['1', '0', '0', '0'])\n",
    "            student = students[index+i]\n",
    "            #problem_ids:该学生答题序列的题目编号列表\n",
    "            problem_ids = student[1]\n",
    "            #correctness:该学生答题序列的结果列表\n",
    "            correctness = student[2]\n",
    "            # print(\"$$$$$$$$$$$$$$$len(problem_ids):\",len(problem_ids))\n",
    "            tmp_print = []\n",
    "            for l in range(len(problem_ids)):\n",
    "                tmp_print.append('('+str(problem_ids[l])+','+str(correctness[l])+')')\n",
    "            answer_print.append(tmp_print)\n",
    "            for j in range(len(problem_ids)-1):\n",
    "                problem_id = int(problem_ids[j])\n",
    "                label_index = 0\n",
    "                if(int(correctness[j]) == 0):\n",
    "                    label_index = problem_id\n",
    "                else:\n",
    "                    label_index = problem_id + num_skills\n",
    "                x[i, j] = label_index\n",
    "                target_id.append(i*num_steps*num_skills+j*num_skills+int(problem_ids[j+1]))\n",
    "                target_correctness.append(int(correctness[j+1]))\n",
    "                actual_labels.append(int(correctness[j+1]))\n",
    "        index += batch_size\n",
    "        count += 1\n",
    "        target_id = torch.tensor(target_id, dtype=torch.int64)\n",
    "        target_correctness = torch.tensor(target_correctness, dtype=torch.float)\n",
    "\n",
    "        # One Hot encoding input data [batch_size, num_steps, input_size]\n",
    "        x = torch.tensor(x, dtype=torch.int64)\n",
    "        x = torch.unsqueeze(x, 2)\n",
    "        input_data = torch.FloatTensor(batch_size, num_steps, input_size)\n",
    "        input_data.zero_()\n",
    "        input_data.scatter_(2, x, 1)\n",
    "#         print(\"######hhm print--input_data.shape=\",input_data.shape)\n",
    "\n",
    "        target_id = target_id.cuda()\n",
    "        target_correctness = target_correctness.cuda()\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            if args.origin_DKT==True:\n",
    "                output, hidden = m(input_data, hidden)\n",
    "            else:\n",
    "                out = m(input_data)\n",
    "            stu_state = out[1]\n",
    "            output = out[0]\n",
    "#             print(\"output.shape=\",output.shape)\n",
    "            output = output.contiguous().view(-1)\n",
    "#             print(\"output.shape=\",output.shape)\n",
    "#             print(\"target_id:\",target_id)\n",
    "            logits = torch.gather(output, 0, target_id)\n",
    "#             print(\"logits:\",logits)\n",
    "            preds = torch.sigmoid(logits)\n",
    "#             print(\"preds:\",preds)\n",
    "            for p in preds:\n",
    "                pred_labels.append(p.item())\n",
    "\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(logits, target_correctness)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "#             torch.nn.utils.clip_grad_norm_(m.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                \n",
    "                if args.origin_DKT==True:\n",
    "                    m.eval()\n",
    "                    output, hidden = m(input_data, hidden)\n",
    "                else:\n",
    "                    out = m(input_data)\n",
    "                stu_state = out[1]\n",
    "                output=out[0]\n",
    "#                 for i in range(output.shape[0]):\n",
    "                \n",
    "                for i in range(stu_state.shape[0]):\n",
    "                    ans_len = len(answer_print[i])\n",
    "                    state_writer.add_embedding(\n",
    "                    mat=stu_state[i][:ans_len,:],#学生的能力状态向量（学生数量*hidden_size）每行表示一个数据\n",
    "                    metadata=answer_print[i],#时间编号，对应时间步num_steps\n",
    "                    global_step=(batch_num-1)*batch_size+i,#第几个学生\n",
    "                    tag=str(epoch) #第几个轮次\n",
    "                )    \n",
    "                output = output.contiguous().view(-1)\n",
    "                logits = torch.gather(output, 0, target_id.cuda())\n",
    "\n",
    "                # preds\n",
    "                preds = torch.sigmoid(logits)\n",
    "                for p in preds:\n",
    "                    pred_labels.append(p.item())\n",
    "\n",
    "                # criterion = nn.CrossEntropyLoss()\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "                loss = criterion(logits, target_correctness)\n",
    "                total_loss += loss.item()\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(actual_labels, pred_labels, pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        total_auc+=auc\n",
    "        r2 = r2_score(actual_labels, pred_labels)\n",
    "    average_auc = total_auc/batch_num\n",
    "    average_loss = total_loss/batch_num\n",
    "    print(\"Epoch: {},   average_AUC: {}, average_loss:{}\".format(epoch,  average_auc,average_loss))\n",
    "    return average_auc,average_loss,actual_labels,pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_data_path = args.train_data_path\n",
    "    test_data_path  = args.test_data_path\n",
    "    batch_size = args.batch_size\n",
    "    concept_num = args.concept_num\n",
    "    qm_data_path = args.qm_data_path\n",
    "    hidden_size = args.hidden_size\n",
    "    \n",
    "    train_students, train_max_num_problems, train_max_skill_num = load_data(train_data_path)\n",
    "    num_steps = train_max_num_problems\n",
    "    num_skills = train_max_skill_num\n",
    "    num_layers = 3\n",
    "    test_students, test_max_num_problems, test_max_skill_num = load_data(test_data_path)\n",
    "\n",
    "#     q_matrix = np.random.rand(num_skills,concept_num)\n",
    "#     for i in range(num_skills):\n",
    "#         row = q_matrix[i]\n",
    "#         s = sum(row)\n",
    "#         for j in range(concept_num):\n",
    "#             q_matrix[i][j] = q_matrix[i][j]/s\n",
    "#     q_matrix = torch.tensor(q_matrix, dtype = torch.float)\n",
    "\n",
    "    q_matrix = np.load(qm_data_path)\n",
    "#     q_matrix = np.loadtxt(qm_data_path,delimiter=',')\n",
    "    q_matrix = torch.from_numpy(q_matrix)\n",
    "    q_matrix = torch.tensor(q_matrix, dtype = torch.float)\n",
    "    print(\"q_matrix.shape:\",q_matrix.shape)\n",
    "#     print(\"q_matrix:\",q_matrix)\n",
    "    \n",
    "    print(\"hhm print---num_steps=\",num_steps)\n",
    "    print(\"hhm print---num_skills=\",num_skills)\n",
    "#     print(\"hhm print--q_matrix:\",q_matrix)\n",
    "    args.origin_DKT = False\n",
    "    # model = DeepKnowledgeTracing('LSTM', num_skills*2, args.hidden_size, num_skills, num_layers)\n",
    "    model = RNN(num_skills*2,hidden_size,num_skills,num_layers)\n",
    "    model = torch.nn.DataParallel(model,device_ids=[0])\n",
    "    model.cuda()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, eps=args.epsilon)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=args.learning_rate)\n",
    "    now=datetime.datetime.now().replace(microsecond=0) \n",
    "    time_now = (now + datetime.timedelta(hours=8))\n",
    "    state_writer = SummaryWriter('stu_state/'+'DKT_e6_s500_q10_kc5'+str(num_layers)+'层')\n",
    "    \n",
    "    test_epoch=0\n",
    "    for i in range(args.epochs):\n",
    "        auc,loss,actual_labels,pred_labels = run_epoch(model, optimizer,  train_students, batch_size, num_steps, num_skills, q_matrix, state_writer, epoch=i)    \n",
    "        # Testing\n",
    "        if ((i + 1) % args.evaluation_interval == 0):\n",
    "            auc,loss,actual_labels,pred_labels = run_epoch(model, optimizer, test_students, batch_size, num_steps, num_skills, q_matrix, state_writer, training=False, epoch=test_epoch)\n",
    "            print('Testing')\n",
    "            test_epoch+=1\n",
    "            # print(rmse, auc, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of rows is 1200\n",
      "The number of students is  400\n",
      "Finish reading data\n",
      "the number of rows is 300\n",
      "The number of students is  100\n",
      "Finish reading data\n",
      "q_matrix.shape: torch.Size([10, 5])\n",
      "hhm print---num_steps= 27\n",
      "hhm print---num_skills= 10\n",
      "Epoch: 0,   average_AUC: 0.5842508194982926, average_loss:0.6990961382786433\n",
      "Epoch: 1,   average_AUC: 0.618450734841445, average_loss:0.6848550140857697\n",
      "Epoch: 2,   average_AUC: 0.6529115860345732, average_loss:0.6634921729564667\n",
      "Epoch: 3,   average_AUC: 0.6806056437806355, average_loss:0.6300158500671387\n",
      "Epoch: 4,   average_AUC: 0.6795801408277891, average_loss:0.6008747766415278\n",
      "Epoch: 0,   average_AUC: 0.6402934133699076, average_loss:0.6133372187614441\n",
      "Testing\n",
      "Epoch: 5,   average_AUC: 0.6758751630018714, average_loss:0.5857284814119339\n",
      "Epoch: 6,   average_AUC: 0.6791199630563926, average_loss:0.5790772835413615\n",
      "Epoch: 7,   average_AUC: 0.6866154564115506, average_loss:0.5757156610488892\n",
      "Epoch: 8,   average_AUC: 0.6907062652454566, average_loss:0.5736572394768397\n",
      "Epoch: 9,   average_AUC: 0.6925042086958815, average_loss:0.5723024209340414\n",
      "Epoch: 1,   average_AUC: 0.6614729726782312, average_loss:0.5975592533747355\n",
      "Testing\n",
      "Epoch: 10,   average_AUC: 0.6935437406820396, average_loss:0.5713647653659185\n",
      "Epoch: 11,   average_AUC: 0.6939847342736182, average_loss:0.5706795006990433\n",
      "Epoch: 12,   average_AUC: 0.6968128741168735, average_loss:0.5701538672049841\n",
      "Epoch: 13,   average_AUC: 0.6972212261653145, average_loss:0.5697307189305624\n",
      "Epoch: 14,   average_AUC: 0.6977249844303207, average_loss:0.5693732996781667\n",
      "Epoch: 2,   average_AUC: 0.671146388389376, average_loss:0.594895084698995\n",
      "Testing\n",
      "Epoch: 15,   average_AUC: 0.6989104951079558, average_loss:0.5690578122933706\n",
      "Epoch: 16,   average_AUC: 0.699066772845911, average_loss:0.5687669018904368\n",
      "Epoch: 17,   average_AUC: 0.6996334892796207, average_loss:0.568482905626297\n",
      "Epoch: 18,   average_AUC: 0.700714836433863, average_loss:0.5681778440872828\n",
      "Epoch: 19,   average_AUC: 0.7026428590199584, average_loss:0.5677846372127533\n",
      "Epoch: 3,   average_AUC: 0.6775354564737919, average_loss:0.5925403436024984\n",
      "Testing\n",
      "Epoch: 20,   average_AUC: 0.7071401684836619, average_loss:0.567079688111941\n",
      "Epoch: 21,   average_AUC: 0.7089646381468557, average_loss:0.5659918685754141\n",
      "Epoch: 22,   average_AUC: 0.7109328947196515, average_loss:0.565059075752894\n",
      "Epoch: 23,   average_AUC: 0.7115828213718342, average_loss:0.564142699042956\n",
      "Epoch: 24,   average_AUC: 0.712146564538302, average_loss:0.5632826288541158\n",
      "Epoch: 4,   average_AUC: 0.6798790904246785, average_loss:0.5875841776529948\n",
      "Testing\n",
      "Epoch: 25,   average_AUC: 0.7133574507751197, average_loss:0.5623453160127004\n",
      "Epoch: 26,   average_AUC: 0.7154107383691581, average_loss:0.5614605148633321\n",
      "Epoch: 27,   average_AUC: 0.7184469375201442, average_loss:0.5604892075061798\n",
      "Epoch: 28,   average_AUC: 0.7206946727252116, average_loss:0.5594963083664576\n",
      "Epoch: 29,   average_AUC: 0.7233836371887278, average_loss:0.5582653482755026\n",
      "Epoch: 5,   average_AUC: 0.6897872504309005, average_loss:0.583251953125\n",
      "Testing\n",
      "Epoch: 30,   average_AUC: 0.7259779951945443, average_loss:0.5567949463923773\n",
      "Epoch: 31,   average_AUC: 0.7293401960719947, average_loss:0.5550598651170731\n",
      "Epoch: 32,   average_AUC: 0.7328652536404979, average_loss:0.5531673232714335\n",
      "Epoch: 33,   average_AUC: 0.7360280425464741, average_loss:0.551346148053805\n",
      "Epoch: 34,   average_AUC: 0.7384891419593984, average_loss:0.5497063050667444\n",
      "Epoch: 6,   average_AUC: 0.7048439514835745, average_loss:0.5746894280115763\n",
      "Testing\n",
      "Epoch: 35,   average_AUC: 0.7402219943003928, average_loss:0.5482504864533743\n",
      "Epoch: 36,   average_AUC: 0.7417773452475388, average_loss:0.5469660858313242\n",
      "Epoch: 37,   average_AUC: 0.7430453699070946, average_loss:0.5457874983549118\n",
      "Epoch: 38,   average_AUC: 0.7443000346032727, average_loss:0.5446866552035013\n",
      "Epoch: 39,   average_AUC: 0.7457184132348776, average_loss:0.543630118171374\n",
      "Epoch: 7,   average_AUC: 0.7183633950649009, average_loss:0.5680947105089823\n",
      "Testing\n",
      "Epoch: 40,   average_AUC: 0.7467918169492055, average_loss:0.5426546980937322\n",
      "Epoch: 41,   average_AUC: 0.7475114158650903, average_loss:0.5417631541689237\n",
      "Epoch: 42,   average_AUC: 0.7484698543429604, average_loss:0.5409427955746651\n",
      "Epoch: 43,   average_AUC: 0.749216287913943, average_loss:0.5401839464902878\n",
      "Epoch: 44,   average_AUC: 0.7497182890677365, average_loss:0.5394763325651487\n",
      "Epoch: 8,   average_AUC: 0.7270887114912448, average_loss:0.563981811205546\n",
      "Testing\n",
      "Epoch: 45,   average_AUC: 0.7502900441972217, average_loss:0.5388213520248731\n",
      "Epoch: 46,   average_AUC: 0.7508803560132865, average_loss:0.5382157564163208\n",
      "Epoch: 47,   average_AUC: 0.7517315815870972, average_loss:0.5376408894856771\n",
      "Epoch: 48,   average_AUC: 0.7526007362372017, average_loss:0.537083350121975\n",
      "Epoch: 49,   average_AUC: 0.7533264913798257, average_loss:0.5365415588021278\n",
      "Epoch: 9,   average_AUC: 0.7280799138717239, average_loss:0.563672145207723\n",
      "Testing\n",
      "Epoch: 50,   average_AUC: 0.7539266078578214, average_loss:0.5360147282481194\n",
      "Epoch: 51,   average_AUC: 0.7546218869935144, average_loss:0.5354975660641988\n",
      "Epoch: 52,   average_AUC: 0.7552363248690273, average_loss:0.5349821796019872\n",
      "Epoch: 53,   average_AUC: 0.7557834458310984, average_loss:0.5344818010926247\n",
      "Epoch: 54,   average_AUC: 0.7563560988909312, average_loss:0.53399774680535\n",
      "Epoch: 10,   average_AUC: 0.7277119099580475, average_loss:0.5638449788093567\n",
      "Testing\n",
      "Epoch: 55,   average_AUC: 0.756961063469401, average_loss:0.5335147231817245\n",
      "Epoch: 56,   average_AUC: 0.7576842893236663, average_loss:0.5330240751306216\n",
      "Epoch: 57,   average_AUC: 0.7583718106387244, average_loss:0.5325165614485741\n",
      "Epoch: 58,   average_AUC: 0.7589939175432115, average_loss:0.5319848929842314\n",
      "Epoch: 59,   average_AUC: 0.7596407132002133, average_loss:0.5314340218901634\n",
      "Epoch: 11,   average_AUC: 0.7262738245191449, average_loss:0.5644617478052775\n",
      "Testing\n",
      "Epoch: 60,   average_AUC: 0.7603561055565411, average_loss:0.5308757995565733\n",
      "Epoch: 61,   average_AUC: 0.7609542976149305, average_loss:0.5303251768151919\n",
      "Epoch: 62,   average_AUC: 0.7615795292877166, average_loss:0.5297906373937925\n",
      "Epoch: 63,   average_AUC: 0.7621425238719075, average_loss:0.529267263909181\n",
      "Epoch: 64,   average_AUC: 0.7628499878182051, average_loss:0.5287392238775889\n",
      "Epoch: 12,   average_AUC: 0.7258686595804114, average_loss:0.5656359990437826\n",
      "Testing\n",
      "Epoch: 65,   average_AUC: 0.7636189067449953, average_loss:0.5282052556673685\n",
      "Epoch: 66,   average_AUC: 0.764275750638777, average_loss:0.5276874701182047\n",
      "Epoch: 67,   average_AUC: 0.7647872865830632, average_loss:0.5271888102094332\n",
      "Epoch: 68,   average_AUC: 0.7653874192523057, average_loss:0.5267055258154869\n",
      "Epoch: 69,   average_AUC: 0.7659829628182894, average_loss:0.5262367849548658\n",
      "Epoch: 13,   average_AUC: 0.7246543559302684, average_loss:0.5671436985333761\n",
      "Testing\n",
      "Epoch: 70,   average_AUC: 0.7665561061506749, average_loss:0.5257890274127325\n",
      "Epoch: 71,   average_AUC: 0.7670670035424014, average_loss:0.5253649155298868\n",
      "Epoch: 72,   average_AUC: 0.7675945292465897, average_loss:0.5249539986252785\n",
      "Epoch: 73,   average_AUC: 0.7680791306193632, average_loss:0.5245396792888641\n",
      "Epoch: 74,   average_AUC: 0.768589688497102, average_loss:0.5241107021768888\n",
      "Epoch: 14,   average_AUC: 0.7216383019832714, average_loss:0.5707802375157675\n",
      "Testing\n",
      "Epoch: 75,   average_AUC: 0.769192917083633, average_loss:0.5236671964327494\n",
      "Epoch: 76,   average_AUC: 0.7696623433659721, average_loss:0.5232182766000429\n",
      "Epoch: 77,   average_AUC: 0.7701767666358599, average_loss:0.52277093877395\n",
      "Epoch: 78,   average_AUC: 0.7707368094918873, average_loss:0.5223254834612211\n",
      "Epoch: 79,   average_AUC: 0.7713431878289372, average_loss:0.5218735933303833\n",
      "Epoch: 15,   average_AUC: 0.7199364137407814, average_loss:0.5729743440945944\n",
      "Testing\n",
      "Epoch: 80,   average_AUC: 0.7720006585988322, average_loss:0.5214100902279218\n",
      "Epoch: 81,   average_AUC: 0.7726636180373899, average_loss:0.5209244191646576\n",
      "Epoch: 82,   average_AUC: 0.7734561818523895, average_loss:0.5204130088289579\n",
      "Epoch: 83,   average_AUC: 0.7739279212799269, average_loss:0.5199552550911903\n",
      "Epoch: 84,   average_AUC: 0.7743364891678629, average_loss:0.5196547185381254\n",
      "Epoch: 16,   average_AUC: 0.7185052899092499, average_loss:0.5752363602320353\n",
      "Testing\n",
      "Epoch: 85,   average_AUC: 0.7749628391397408, average_loss:0.5193723936875662\n",
      "Epoch: 86,   average_AUC: 0.775793139151468, average_loss:0.5188637549678484\n",
      "Epoch: 87,   average_AUC: 0.7763484267100739, average_loss:0.5182546402017275\n",
      "Epoch: 88,   average_AUC: 0.7765757000685894, average_loss:0.518030638496081\n",
      "Epoch: 89,   average_AUC: 0.7767512534851279, average_loss:0.5178631568948427\n",
      "Epoch: 17,   average_AUC: 0.7159458929214612, average_loss:0.5771397749582926\n",
      "Testing\n",
      "Epoch: 90,   average_AUC: 0.7777571660252396, average_loss:0.5176646411418915\n",
      "Epoch: 91,   average_AUC: 0.778573334624126, average_loss:0.5170119677980741\n",
      "Epoch: 92,   average_AUC: 0.7791519343194678, average_loss:0.5163645471135775\n",
      "Epoch: 93,   average_AUC: 0.7791589059684938, average_loss:0.5161558538675308\n",
      "Epoch: 94,   average_AUC: 0.7790737510656426, average_loss:0.5162405843536059\n",
      "Epoch: 18,   average_AUC: 0.7123695270378846, average_loss:0.579881509145101\n",
      "Testing\n",
      "Epoch: 95,   average_AUC: 0.7798261481323001, average_loss:0.5160873507459959\n",
      "Epoch: 96,   average_AUC: 0.7809191166431311, average_loss:0.515198844174544\n",
      "Epoch: 97,   average_AUC: 0.7816868025144537, average_loss:0.5145329435666403\n",
      "Epoch: 98,   average_AUC: 0.7809028478872047, average_loss:0.5149234756827354\n",
      "Epoch: 99,   average_AUC: 0.7806537979360012, average_loss:0.5149898404876391\n",
      "Epoch: 19,   average_AUC: 0.7113162215252272, average_loss:0.5807921687761942\n",
      "Testing\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
